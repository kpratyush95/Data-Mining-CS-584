# -*- coding: utf-8 -*-
"""Validation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v-baGZPSpwAZk3FEjX_fmkk9TAbU8Jip
"""

import nltk
 nltk.download('punkt')
 nltk.download('stopwords')

# Importing Libraries

import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from nltk.stem.porter import PorterStemmer as PS
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt

# Reading data from files

train_data = pd.read_csv('./Train_data.txt',sep ='\t')
with open("./Test_data.txt", encoding = "utf-8") as fin: test_data = pd.DataFrame(fin.readlines())

# droping missing text labels in training dataset to reduce noise
train_data = train_data.dropna()
train_data = train_data.reset_index(drop=True)
test_data.fillna("none", inplace=True)

train_reviews = train_data['review']
train_sentiment = train_data['sentiment']
test_review = test_data[0]

"""
 Preprocessing the data
 Parameters : Raw data i.e, training and testing reviews
 Output : Cleaned data.
""" 
def data_preprocessing (data):
  tokens = word_tokenize(data)
  words = [token for token in tokens if token.isalpha()]
  no_integers = [x for x in words if not isinstance(x, int)]
  porter = PS()
  stemmed = [porter.stem(word) for word in no_integers]
  stop_words = stopwords.words('english')
  words_new = [word for word in stemmed if word not in stop_words]
  tokens = [w.lower() for w in words_new]
  cleaned_sentence = " "
  cleaned_sentence = cleaned_sentence.join(tokens)
  return cleaned_sentence

# cleaning the data.
def cleaning_data(data):
    cleaned_data = []
    for i in range(len(data)):
      cleaned_data.append(data_preprocessing(data.iloc[i]))
    return cleaned_data

cleaned_train_reviews = cleaning_data(train_reviews)

cleaned_test_reviews = cleaning_data(test_review)

# X_train, X_test, y_train, y_test = train_test_split(train_reviews, train_sentiment, test_size=0.2, random_state=0, shuffle= False)

# K cross validataion, splitting the training data for testing 
kf = KFold(n_splits=10)
for train_index, test_index in kf.split(cleaned_train_reviews):
   X_train, X_test = cleaned_train_reviews[train_index[0]:train_index[-1]], cleaned_train_reviews[test_index[0]:test_index[-1]]
   y_train, y_test = train_sentiment[train_index[0]:train_index[-1]], train_sentiment[test_index[0]:test_index[-1]]

# Vectorizing the data and converting it into sparse matrices by using TFIDF vectorizer
vectorizer = TfidfVectorizer(stop_words= 'english', max_df = 1.0, min_df = 0.0015, max_features = 1000)
train_tfidf_matrix = vectorizer.fit_transform(X_train)
test_tfidf_matrix = vectorizer.transform(X_test)

"""
Calculating the K-nearest neighbors
Parameters : 
            K- value of how many nearest neighbors you want, preferrably odd.
            test_tfidf_matrix : Spase matrix of TFIDF of test reviews
            train_tfidf_matrix : Spase matrix of TFIDF of train reviews
            reviews : test reviews
            sentiments : train sentiments.
            The neighbors distance is calculated using cosine similarity.
Output: a dataframe containg the predicted sentiments for each test reviews based on its neighbors. 
"""
def k_nearest_neighbors(k, test_tfidf_matrix, train_tfidf_matrix, reviews, sentiments):
    test_reviews = []    
    for index, line in enumerate(reviews):
        # cosine similarity
        cos_similarity = cosine_similarity(test_tfidf_matrix[index], train_tfidf_matrix).flatten()
        # getting the indices of K nearest neighbors      
        neighbor_indices = cos_similarity.argsort()[:-k:-1]
        neighbor_list = sentiments[neighbor_indices].tolist()
        # classify prediction review based on neighbor sum
        neighbor_sum = sum(neighbor_list)
        if neighbor_sum > 0:
          test_reviews.append("+1")
        else:
          test_reviews.append(-1)
            
    return pd.DataFrame(test_reviews)

#Calculating the accuracy of the predictions based on train test split. 
Accuracy = []
for i in range(1, 511, 2):
  print(i)
  y_pred = k_nearest_neighbors(i,test_tfidf_matrix, train_tfidf_matrix, X_test, y_train)    
  acc = accuracy_score(y_test, y_pred.astype(int))
  Accuracy.append((i, acc))
  print(i, acc)
#test_reviews = kNN(25, test_tfidf_matrix, train_tfidf_matrix, X_test, y_train)

X_axis = [x[0] for x in Accuracy]
Y_axis = [x[1] for x in Accuracy]
plt.plot(X_axis, Y_axis, color='r')
plt.xlabel('K- Value')
plt.ylabel('Accuracy')
plt.show()

