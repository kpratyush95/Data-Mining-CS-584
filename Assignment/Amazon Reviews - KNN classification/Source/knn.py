# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17X7RHT0l7BJXPsNRuZ3qZlxeq9d-yGbM
"""

# import nltk
#  nltk.download('punkt')
#  nltk.download('stopwords')

# Importing Libraries
import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn.metrics.pairwise import cosine_similarity
from nltk.stem.porter import PorterStemmer as PS

# Reading data from files
# train_data = pd.read_csv("Train_data.txt",sep ='\t')
with open("./Train_data.txt", encoding = "utf-8") as fin: train_data = pd.DataFrame(fin.readlines())

with open("./Test_data.txt", encoding = "utf-8") as fin: test_data = pd.DataFrame(fin.readlines())

# droping missing text labels in training dataset to reduce noise
train_data = train_data.dropna()
train_data = train_data.reset_index(drop=True)
test_data.fillna("none", inplace=True)

train_reviews = train_data['review']
train_sentiment = train_data['sentiment']
test_review = test_data[0]

"""
 Preprocessing the data
 Parameters : Raw data i.e, training and testing reviews
 Output : Cleaned data.
""" 
def data_preprocessing ( data):
  tokens = word_tokenize(data)
  words = [token for token in tokens if token.isalpha()]
  no_integers = [x for x in words if not isinstance(x, int)]
  porter = PS()
  stemmed = [porter.stem(word) for word in no_integers]
  stop_words = stopwords.words('english')
  words_new = [word for word in stemmed if word not in stop_words]
  tokens = [w.lower() for w in words_new]
  cleaned_sentence = " "
  cleaned_sentence = cleaned_sentence.join(tokens)
  return cleaned_sentence

# cleaning the data.
def cleaning_data(data):
    cleaned_data = []
    for i in range(len(data)):
      cleaned_data.append(data_preprocessing(data.iloc[i]))
    return cleaned_data

cleaned_train_reviews = cleaning_data(train_reviews)

cleaned_test_reviews = cleaning_data(test_review)

# Vectorizing the data and converting it into sparse matrices by using TFIDF vectorizer
vectorizer = TfidfVectorizer(stop_words= 'english', max_df = 1.0, min_df = 0.01, max_features = 1000)
train_tfidf_matrix = vectorizer.fit_transform(cleaned_train_reviews)
test_tfidf_matrix = vectorizer.transform(cleaned_test_reviews)

"""
Calculating the K-nearest neighbors
Parameters : 
            K- value of how many nearest neighbors you want, preferrably odd.
            test_tfidf_matrix : Spase matrix of TFIDF of test reviews
            train_tfidf_matrix : Spase matrix of TFIDF of train reviews
            reviews : test reviews
            sentiments : train sentiments.
            The neighbors distance is calculated using cosine similarity.
Output: a dataframe containg the predicted sentiments for each test reviews based on its neighbors. 
"""
def k_nearest_neighbors(k, test_tfidf_matrix, train_tfidf_matrix, reviews, sentiments):
    test_reviews = []    
    for index, line in enumerate(reviews):
        # cosine similarity
        cos_similarity = cosine_similarity(test_tfidf_matrix[index], train_tfidf_matrix).flatten()
        # getting the indices of k nearest neighbors  
        neighbor_indices = cos_similarity.argsort()[:-k:-1]
        neighbor_list = sentiments[neighbor_indices].tolist()
        neighbor_sum = sum(neighbor_list)
        # classify prediction review based on neighbor sum
        if neighbor_sum > 0:
          test_reviews.append("+1")
        else:
          test_reviews.append(-1)
            
    return pd.DataFrame(test_reviews)

test_reviews = k_nearest_neighbors(149, test_tfidf_matrix, train_tfidf_matrix, cleaned_test_reviews, train_sentiment)

# Writing the output in a text file for miner submission.
with open('./output.txt', 'a') as f: f.write(test_reviews.to_string(header = False, index = False))

